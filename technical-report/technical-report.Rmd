---
title: "Building a Forecast Hub: Feedback after 6 months of running the European COVID-19 Forecast Hub"
bibliography: references.bib
biblio-style: unsrt
output: 
  rmarkdown::pdf_document:
    keep_tex: false
    latex_engine: xelatex
mainfont: Fira Sans Light
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, cache.extra = Sys.Date())
```

```{r global-vars}
gh_repo <- "epiforecasts/covid19-forecast-hub-europe"
```

The European COVID-19 Forecast Hub was launched in early 2021 and after six months, time has come for us to gather some feedback about what worked and what did not. For reasons detailed later, it is difficult to make major changes in the hub while it is currently running but we want to make sure our experience allows the next forecast hub to be better. The COVID-19 European Forecast Hub is by no means the first forecast hub and it already benefited greatly from the US COVID-19 Forecast Hub, the Germany/Poland COVID-19 Forecast Hub, as well as the US influenza hub. We however believe that our fresh eyes on the already existing alternatives put us in a perfect place to comment on challenges encountered when a new team tries to build a forecast hub and why tools that worked well for a specific team might create issues for others. Additionally, compared to influenza hub, the novelty, the urgency and the impact of the COVID-19 generated an amount of contributions unseen until then, thus generating new challenges.

This report focuses on three areas that caused trouble during these 6 months the hub has been running:

- the problems for the forecasters
- the problems for the tools
- the problems for the maintainers

# The problems for the forecasters: improving user-friendliness

```{r}
library(gh)
library(purrr)
library(magrittr)

pulls <- gh::gh("/repos/{gh_repo}/pulls?state=all", gh_repo = gh_repo)

contributors <- gh::gh("/repos/{gh_repo}/contributors", gh_repo = gh_repo, .limit = Inf) %>%
  map_chr("login")
contributors <- contributors[contributors != "actions-user"]

get_first_pr <- function(user) {
 
  q <- glue::glue('
  {
    user(login: "<<user>>") {
      pullRequests(first:1) {
        edges {
          node {
            url
          }
        }
      }
    }
  }', user = user, .open = "<<", .close = ">>")

  dat <- gh::gh_gql(q)
  
  return(dat[[1]]$user$pullRequests$edges[[1]]$node$url)
}

first_prs <- map_chr(contributors, get_first_pr) 

nb_new <- first_prs %>%
  stringr::str_starts(glue::glue("https://github.com/{repo}", repo = gh_repo)) %>%
  sum()
```

The European COVID-19 Forecast Hub uses GitHub as its platform to submit forecasts. Each week, participating teams submit a pull request with their forecasts for the coming weeks. As developers, it is easy to underestimate how complex this process can be for new users. Indeed, `r glue::glue("{percent_new}% ({nb_new}/{nb_total})", percent_new = round(nb_new/length(contributors)*100), nb_total = length(contributors))` of contributors to the hub were new GitHub users that made their first ever pull request in our repository.

Although GitHub offers a more intuitive interface to git, we noticed some patterns that were common sources of issues for new contributors: starting from branches out-of-sync with upstream or uploading extra files (which is often a symptom of using `git commit -a` or `git add *`). Most of the time, their preferred option to solve these issue was to close the pull request and open another one.

To limit friction and based on the experience of the US hub, we decided to always use a merge commit instead of rebasing on top of the current `HEAD`. This solves the case where users add their changes each week on the same branch instead of creating a new one. However, this has some consequences on the clarity of the git history, as detailed later.

Since forecasters will most of the time only add new files each week, and rarely have to modify or delete files, a simple submission web form could be used instead of the current system.

# The problems faced by tools

## GitHub & git

```{r}
repo_stats <- gh::gh("/repos/{gh_repo}", gh_repo = gh_repo)
```

Beyond the issue of user-friendliness mentioned previously, git is primarily designed to work with code and plain text, not with data. Storing forecast data in the GitHub repository lead to a huge inflation of the repository size. At the time of this writing, the repository size lies at `r glue::glue("{repo_stats$size %/% 1024}MB")` while GitHub recommended limit is 1GB [^github-limit].

[^github-limit]: "We recommend repositories remain small, ideally less than 1 GB, and less than 5 GB is strongly recommended." Source: https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github.

Storing data directly in the repository also lead to a messy git history. When looking at the output of `git log` or when running `git bisect`, it became very difficult to find the relevant code changes. This was made even worse by the fact that we use merge commits to include new forecasts, a mitigation measure mentioned previously to limit conflicts in weekly submission.

Both these problems weigh strongly in favour of a different venue to store forecast data. A simple change would be to have a distinct git repository for the forecast data. This is the choice other projects with a user-contributed database adopted [^zotero] or wish they had adopted [^https-everywhere]. An even better solution is to use a dedicated storage platform such as [zoltar](https://www.zoltardata.com/). This is related to the need for modularity that we develop later in this report.

[^zotero]: https://github.com/zotero/translators
[^https-everywhere]: https://github.com/EFForg/https-everywhere/issues/2697

## Speed

Somewhat related to the size issue, we also faced problems of performance, both in terms of display for the visualisation app, and in terms of computation time for the reports.

Beyond the obvious option to optimize the relevant portions of the code, these performance issues can be addresses in two ways:

- long running server side computations, such as the rendering of the weekly reports, could be improved to run in parallel and then moved to a computing cluster, instead of using GitHub Actions
- slow display can be solved by using a content delivery network (CDN), which is the standard method to deliver heavy resource on the web efficiently.

# Maintainability

## The challenges of development in production

While running the forecast hub, we are faced by one difficulty that research software seldomly encounter. We are running a complex machinery in production, meaning that any change could immediately impact contributing teams. This problem is usually partially mitigating in traditional software through versioning. Changes in a piece of software results in a new version, that users are more or less free to install whenever convenient. Versioning is not possible in the case of the hub: all contributors have to deal with the same version. Even purely technical changes (like changing the format of the metadata file) requires an action from users to avoid git conflicts. For this reason, and for the sake of contributing teams, we sometimes had to delay or give up on technical improvements.

A possible way to reduce the friction caused on users by technical changes would be to build an intermediate layer between the hub core and contributors. The contributors would only be exposed to this simple and stable intermediate layer. And maintainers of the hub would then be free to do any kind of changes to the core.
This is not the case in the current iteration where there is a complete overlap between user input (weekly forecasts as csv and metadata file as yaml) and its internal representation in the hub machinery. The use of zoltar, mentioned earlier, would here again resolve this issue: the internal storage would be on zoltar, but any system could be used for forecast submission (including GitHub if we overlook the user-friendliness issue) with a series of scripts connecting the two.

## Modular approach

Perhaps one of the most important change recommended in this report is to adopt a modular approach rather than an integrated approach. The current implementations of forecast hubs, including the European COVID-19 Forecast Hub, have strongly integrated approaches, where all the various pieces are interconnected. Integrated approaches are often easier to develop at first since it is not necessary to handle communication between the various pieces present in a modular approach. However, a modular approach facilitates modifications later down the line. Similarly, an integrated approach forces an all-or-nothing adoption of a technical solution. It is not possible to take some pieces and leave out others. On the opposite, a modular approach allows exchange of pieces between teams and collaboration despite having made different choices in some places.
Additionally, a modular approach, allows testing of the individual pieces, which is crucial for a project that is directly in production [^ci-validations].

[^ci-validations]: https://github.com/epiforecasts/covid19-forecast-hub-europe-validations/issues/8

## Hardcoded values

On a similar note, the data and metadata files should be modular by including a single piece of information in each column [^split-cols] or each field [^split-fields]. This facilitates manipulation and reformatting in downstream scripts or in the future.

For the forecast data, we currently store three bits of information in one column (`inc case 4wk ahead`): the target variable (cases, deaths, etc.), its type (cumulated or incident), and the horizon. A good argument that these should be separated in several columns is the fact that this is automatically done by `covidHubUtils::load_forecasts()`, which is the first step of all our downstream scripts. Beyond ease of use, this has practical impact because distincts columns would allow us to spell out the target variable (e.g., hospitalisations instead of hosps), which is a necessary step to make our script target-agnostic [^spell].

This problem also became visible for the metadata file when we tried to isolate the email addresses of the participants. Since they were added within the `model_contributors` field, alongside the contributors' names, we had to rely on regular expressions to try and isolate the email addresses, or even resort to manual edition when there was no simple pattern to extract. This was still possible in our case because we have a limited number of participant but this approach does not scale.

[^split-cols]: https://github.com/epiforecasts/covid19-forecast-hub-europe/issues/708
[^split-fields]: https://github.com/epiforecasts/covid19-forecast-hub-europe/pull/1016
[^spell]: https://github.com/epiforecasts/covid19-forecast-hub-europe/issues/796

## Diversity of programming languages

It is not realistic to expect to everyone (or really anyone) in the development
team will have a deep understanding of more a couple of programming languages.
Therefore, the use of multiple programming languages forces us to have a dedicated
developer for each part of the hub, with no one having an overall understanding or
overview of the whole project. It also increases dependency to a single individual,
which might be otherwise committed in a time of need. This is akin to the problem of [bus factor](https://en.wikipedia.org/wiki/Bus_factor), common in software development. All software projects, including the forecast hubs, should try to increase the bus factor by using a limited number of programming languages and frameworks.


